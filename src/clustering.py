#!/usr/bin/python
import sys
import mixture as mx
import numpy as np
import Pycluster as pc
import random

import divergence as dv
import value_counts as vc
import preprocess as prep
import copy
import os
import itertools as itt
import math
import mycluster as mc
import plot

# load initial values from generated by k-means
def load_means(fname):
  f = open(fname, 'r')
  
  means = {}
  for line in f:
    row = line.split()
    means[row[0]] = [float(x) for x in row[1:]]

  return means

def examine_kmeans_results(k, table, labels):
  r = table.shape[1]
  acc = [[0.0]*r] * k
  mi = [[1e7]*r] * k
  ma = [[-1]*r] * k
  cnt = [0] * k

  for idx, row in enumerate(table):
    label = labels[idx]
    acc[label] = [cur_acc + float(cur_val) for cur_acc, cur_val in itt.izip(acc[label], row)]
    mi[label] = [min(cur_mi, float(cur_val)) for cur_mi, cur_val in itt.izip(mi[label], row)]
    ma[label] = [max(cur_ma, float(cur_val)) for cur_ma, cur_val in itt.izip(ma[label], row)]
    cnt[label] += 1
  
  for j in range(r):
    for i in range(k):    
      print i, acc[i][j]/cnt[i], cnt[i], mi[i][j], ma[i][j]

def get_centers(table, labels):
  sums = {}
  cnts = {}
  minb = {}
  maxb = {}

  for i, row in enumerate(table):
    label = labels[i]
    if label not in sums:
      sums[label] = [0.0] * len(row)
      minb[label] = [-1] * len(row)
      maxb[label] = [-1] * len(row)

    if label not in cnts:
      cnts[label] = 0

    for j, cell in enumerate(row):
      tmp = float(cell)
      sums[label][j] += tmp
      if minb[label][j] < 0 or tmp < minb[label][j]:
        minb[label][j] = tmp
      if maxb[label][j] < 0 or tmp > maxb[label][j]:
        maxb[label][j] = tmp

    cnts[label] += 1

  centers = {}
  for k, v in sums.items():
    centers[k] = [x / cnts[k] for x in v]

  for c in centers:
    print 'cluster:' 
    print '\n'.join(str(x) + ' ' + str(y) + ' ' + str(z) + ' ' + str(z-y) 
                for x, y, z in itt.izip(centers[c], minb[c], maxb[c]))

  return centers

def euc_distance(row1, row2):
  return math.sqrt(sum((u-v) * (u-v) for u, v in itt.izip(row1, row2)))

def manhattan_distance(row1, row2):
  return math.sqrt(sum(math.fabs(u-v) for u, v in itt.izip(row1, row2)))

def assign_center(row, centers, dfun = 'm'):
  row = [float(x) for x in row]
  min_dist = None
  assign = None

  for k, c in centers.items():
    if dfun == 'm':
      dist = manhattan_distance(row, c)
    elif dfun == 'e':
      dist = euc_distance(row, c)
    else:
      print 'unknown distance function %s' % dfun

    if not min_dist or min_dist > dist:
      min_dist = dist
      assign = k
  return assign

def kmeans(k, table):
 # k = 50
  (labels, error, nfound) = pc.kcluster(table, k, None, None, 0, 20, 'a', 'b')
#  plot.plot_scatter(table, labels, k)
  
#  centers = get_centers(table, labels)
#  np.random.shuffle(table)
#  tab = [map(float, x) for x in table[:1000]]

#  mycluster = mc.MyClustering(tab, k)
#  mycluster.init_heap()
#  mycluster.hierarchy_cluster()
#  mycluster.clear_sample_points()

#  for i, row in enumerate(table):
#    if i % 1000 == 0:
#      print 'progress: %d' % i
#    mycluster.add_point(i, map(float, row))

#  mycluster.get_cluster()

  return labels

def classify_data_kmeans(k, cols, path, centers):
  for f in prep.gen_file_list(path):
    if f.endswith('.train'):
      print 'classifying %s' % f
      
      fw = open(f[:f.rfind('/')] + '/.' + str(k) + '.labels', 'w')
      prog = 0
      for row in prep.gen_file_stream(f, cols):
        if prog % 10000 == 0:
          print 'progress: %d' % prog
        label = assign_center(row, centers)
        fw.write(str(label) + '\n')
        prog += 1
      fw.close()

#Main clustering procedure
# return a list of labels, one for each row
def clustering(k, feature_cols, feature_domains, header, table, seeds, result_file):
  best_loglike = None
  best_model = None
  # Giant random seeding loop, 

  data = mx.DataSet()
  data.fromArray(table)
  for r in range(1):
  #  weights = np.random.random_sample(k)
  #  weights_norm = weights / sum(weights)
    weights_norm = [1.0/k] * k
    components = []
    for i in range(k):
      products = []
      for j in range(table.shape[1]):
        col_type = prep.get_col_type(feature_cols[j], header)
        col_id = feature_cols[j]

        if col_type == 'cat':
          vals = feature_domains[col_id].keys()
          cnt_vals = len(vals)
          rand_dist = np.random.random_sample(cnt_vals)

          dist = mx.DiscreteDistribution(cnt_vals, rand_dist / sum(rand_dist), mx.Alphabet(vals))
        
        elif col_type == 'num':
          min_val = feature_domains[col_id]['min']
          max_val = feature_domains[col_id]['max']
      #  mean = random.uniform(min_val, max_val)
          mean = seeds[header[col_id][0]][i]
          stdev = (max_val - min_val) / 2.0 / k
          
          dist = mx.NormalDistribution(mean, stdev)

        else:
          sys.exit(1)
        products.append(dist)

      comp = mx.ProductDistribution(products)
      components.append(comp)
    
    mix_table = mx.MixtureModel(k, weights_norm, components)
    print mix_table

    #loglike = mix_table.randMaxEM(data,1,50,50)
    #print loglike
    #print mix_table
    if not best_loglike or loglike > best_loglike:
    #  best_loglike = loglike
      best_model = copy.copy(mix_table)

#data.internalInit(mix)
# mix_table.modelInitialization(data)
#  print best_loglike
#  print best_model

  labels = best_model.classify(data, None, None, 1) 

  ## output clustering results
  
  # count cluster sizes on sampled data
  f = open(result_file + '.stats', 'w')
  cnt = {}
  for l in labels:
    cnt[l] = 1 if l not in cnt else cnt[l] + 1

  for l in cnt:
    f.write('%s %d %f%%\n' % ( l, cnt[l], cnt[l] * 100.0 / sum(cnt.values())))
  f.close()

  mx.writeMixture(best_model, result_file + '.model')
  return best_model

def assign_labels(model, data_file, cols):
  table = []
  labels = []
 
  cnter = 0
  for row in prep.gen_file_stream(data_file, cols):
    table.append(row)
    cnter += 1
    if cnter % 100000 == 0:
      print cnter
      data = mx.DataSet()
      print 'table size: %d' % len(table)
      data.fromArray(np.array(table))
      labels += list(model.classify(data, None, None, 1))
      del data
      table[:] = []
    #  print len(list(labels))

  # process the trailing entries
  if len(table) > 0:
    data = mx.DataSet()
    data.fromArray(np.array(table))
    labels += list(model.classify(data, None, None, 1))

  return labels


# Assign labels to all the '*.train' data in [path] using 'model'.
# The labels will be written into file .'k'.labels in the same folder.
# So put at most one train file in each folder :(
def classify_data(k, cols, model, path):
  for f in prep.gen_file_list(path):
    if f.endswith('.train'):
      print 'classifying %s' % f

      fw = open(f[:f.rfind('/')] + '/.' + str(k) + '.labels', 'w')
      labels = assign_labels(model, f, cols)
      fw.write('\n'.join(str(x) for x in labels) + '\n')
      fw.close()

def output(filename, arr):
  f = open(filename, 'w')
  f.write(header + ',class class\n')
  for i, val in enumerate(arr):
    f.write(','.join(list(arr[i,:])) + '\n')
  f.close()

# cluster all tables
# tailored for tpch
def cluster_all_tables(data_path):
  for d in os.listdir(data_path):
    if not os.path.isdir(data_path + '/' + d):
      continue
   
    if d != 'lineitem':
      continue

    print 'processing %s' % d
    full_path = data_path.rstrip('/') + '/' + d.rstrip('/') + '/' 
    sample_ratio = int(open(full_path + '.ratio').read())
    data_file = '%s%s.train.%d.sample' % (full_path, d, sample_ratio)

    k = int(open(full_path + '.k').read())
    if k > 1:
      feat_cols = prep.get_feature_columns(full_path + '.columns')
      table = prep.load_file(data_file, feat_cols)
      seeds = load_means(full_path + '/.means')
#    output_weka(table, 'weka.arff')
#      return
      feat_doms = prep.read_domains(feat_cols, full_path + '.domains')
      header = prep.get_header(full_path + '.header')

      print 'start clustering %s' % data_file
  #    model = clustering(k, feat_cols, feat_doms, header, table, seeds, data_file + '.res')
   
      labels = kmeans(k, table)
      centers = get_centers(table, labels)
      classify_data_kmeans(k, feat_cols, full_path, centers)
      
# model = mx.readMixture('train/lineitem/lineitem.train.100.sample.res.model')
#      print model
  #try new cluster algo
  #    classify_data(k, feat_cols, model, full_path)

def output_weka(table, fname):
  fout = open(fname, 'w')
  for row in table:
    fout.write(','.join(row) + '\n')
  fout.close()

def main():
  #cols = [schema["dIndustry nominal"], schema["iYearsch nominal"], schema["iMeans nominal"]]
  #cols = sorted([11, 24, 28, 37, 33, 51, 45, 49, 53, 32, 86, 103, 101, 104, 114, 118, 135])
  #cols = sorted([11, 24, 28, 37, 33, 51, 32, 86, 103, 101, 104, 114, 135])
  #all_cols = range(table.shape[1])

#  if len(sys.argv) < 4:
#    print 'Usage: python clustering.py [train_file] [test_data_path] [k]'
#    return

  data_path = sys.argv[1]  
  cluster_all_tables(data_path)

#  table = np.hstack((table, np.array(labels)[np.newaxis].T))
#  output(sys.argv[3], table)
#  dv.calculate_divergence(table, all_cols)

if __name__ == '__main__':
  main()
